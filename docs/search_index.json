[["hypotest.html", "8 Hypothesis Testing 8.1 Overview 8.2 Load packages and dataset 8.3 One-Sample T Test 8.4 Correlated Groups T Test 8.5 Independent Groups T Test 8.6 One-Way Between-Subjects ANOVA 8.7 Two-Way Between-Subjects ANOVA 8.8 Correlation 8.9 Regression", " 8 Hypothesis Testing 8.1 Overview This section guides you through the steps to conduct the various hypothesis tests we cover in class. This only serves as an overview for students who are interested to read ahead. I will go through the hypothesis tests in more detail in class. We will use the hypothetical dataset that I introduced in the previous section. Please download the dataset to follow along here: SWB.csv. 8.2 Load packages and dataset # Load packages library(tidyverse) # Load dataset data &lt;- read_csv(&quot;SWB.csv&quot;) 8.3 One-Sample T Test We use a one-sample t test when we want to compare the data from one group to some hypothesized mean. So, let’s say we’re interested to know whether on average, in 2019, people were more satisfied with their lives than the neutral value of 4. (In this data set, the satisfaction with life items were measured on a 7-point scale, where 4 is the neutral point.) # Calculate the average satisfaction with life score in 2019 for each participant data &lt;- data %&gt;% mutate(swls2019_avg = rowMeans(across(c(swls2019_1:swls2019_5)))) # Conduct the one-sample t test to compare the satisfaction with life score in 2019 against the neutral point of 4. # &quot;two-sided&quot; means we&#39;re conducting a two-tailed test # mu = 4 refers to the hypothesized value we&#39;re comparing to # paired = FALSE means that each observation comes from different individuals (i.e., they are not &quot;paired&quot;) # conf.level = 0.95 means we want the 95% confidence level. Usually, if we use alpha = .05, then the confidence level is 0.95. t.test(data$swls2019_avg, alternative = c(&quot;two.sided&quot;), mu = 4, paired = FALSE, conf.level = 0.95) ## ## One Sample t-test ## ## data: data$swls2019_avg ## t = 8.5309, df = 342, p-value = 4.775e-16 ## alternative hypothesis: true mean is not equal to 4 ## 95 percent confidence interval: ## 4.271882 4.434823 ## sample estimates: ## mean of x ## 4.353353 The results indicate that the average of swls2019_avg is 4.35. When we compare 4.35 to the neutral point of 4, the resulting t value is 8.53. With degrees of freedom 342, the p value is very small, at 4.77e-16. 4.77e-16 is scientific notation expressing 4.77 x 10^-16. So 4.77e-16 would be 0.000000000000000477. The 95% confidence interval is [4.27, 4.43] which is interpreted as follows: We are 95% confident that the true 2019 satisfaction with life score is between 4.27 and 4.43. 8.4 Correlated Groups T Test Suppose we want to know whether people’s satisfaction with life scores changed from 2019 to 2021. To do this, we conduct a correlated groups t test. # Calculate the average satisfaction with life score in 2021 for each participant. # If you did not calculate the average satisfaction with life score in 2019 earlier, you might want to do that here also. data &lt;- data %&gt;% mutate(swls2021_avg = rowMeans(across(c(swls2021_1:swls2021_5)))) # Conduct the correlated groups t test to compare the satisfaction with life score in 2019 and that in 2021 to see if there is any change over the two years. # &quot;two-sided&quot; means we&#39;re conducting a two-tailed test # mu = 0 means we&#39;re comparing the difference against the value of 0. # paired = TRUE means that each individual offered pairs of observations (i.e., swls2019_avg and swls2021_avg) # conf.level = 0.95 means we want the 95% confidence level. Usually, if we use alpha = .05, then the confidence level is 0.95. t.test(data$swls2019_avg, data$swls2021_avg, alternative = c(&quot;two.sided&quot;), mu = 0, paired = TRUE, conf.level = 0.95) ## ## Paired t-test ## ## data: data$swls2019_avg and data$swls2021_avg ## t = 27.83, df = 342, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## 0.4649322 0.5356509 ## sample estimates: ## mean difference ## 0.5002915 The results indicate that the average of the difference between swls2019_avg and swls2021_avg is 0.50. When we compare this difference against 0, the resulting t value is 27.83. With degrees of freedom 342, the p value is very small, at 2.2e-16. The 95% confidence interval is [0.46, 0.54] which is interpreted as follows: We are 95% confident that the true difference between the 2019 and 2021 satisfaction with life scores is between 0.46 and 0.54. 8.5 Independent Groups T Test Suppose we want to know whether men and women differ in satisfaction with life in 2019. To do this, we conduct a independent groups t test. # First, we need to convert gender into a factor data$gender &lt;- factor(data$gender, levels = c(0, 1), labels = c(&quot;male&quot;, &quot;female&quot;)) # Next, we conduct the independent groups t test # Notice that this t.test code takes the form of DV ~ IV # &quot;two-sided&quot; means we&#39;re conducting a two-tailed test # mu = 0 means we&#39;re comparing the difference against the value of 0. # var.equal = TRUE means that we&#39;re assuming homogeneity of variance is met (if homogeneity of variance is violated, use var.equal = FALSE and R will conduct Welch corrections) # conf.level = 0.95 means we want the 95% confidence level. Usually, if we use alpha = .05, then the confidence level is 0.95. t.test(data$swls2019_avg ~ data$gender, alternative = c(&quot;two.sided&quot;), mu = 0, var.equal = TRUE, conf.level = 0.95) ## ## Two Sample t-test ## ## data: data$swls2019_avg by data$gender ## t = -6.8986, df = 341, p-value = 2.564e-11 ## alternative hypothesis: true difference in means between group male and group female is not equal to 0 ## 95 percent confidence interval: ## -0.6891325 -0.3833437 ## sample estimates: ## mean in group male mean in group female ## 4.079762 4.616000 The results indicate that the average satisfaction with life score for those who identify as male is 4.08 and 4.62 for those who identify as female. When we compare the difference between the two genders against 0, the resulting t value is -6.90. With degrees of freedom 341, the p value is very small, at 2.564e-11. The 95% confidence interval is [-0.69, -0.38] which is interpreted as follows: We are 95% confident that the true difference in 2019 satisfaction with life score between males and females is between -0.69 and -0.38. Note. In calculating the t statistic, R took male - female (since 0 = male and 1 = female, and R takes the group with the smaller number and subtracts the group with the larger number) . Because males have a smaller 2019 SWLS than females, the t statistic is negative. However, if R had taken female - male, the t statistic would be positive. Whether it is positive or negative doesn’t matter so long as you know which group has a higher mean. 8.6 One-Way Between-Subjects ANOVA Suppose we want to know whether marital status affects subjective wellbeing. In essence, we might want to find out whether there is at least one mean difference between: a) married vs divorced, b) married vs widowed, c) divorced vs widowed. # Convert marital status into factor data$marital_status &lt;- factor(data$marital_status, levels = c(1, 2, 3), labels = c(&quot;married&quot;, &quot;divorced&quot;, &quot;widowed&quot;)) # Run the ANOVA # ANOVA is strange in that to view the results, you need to save it as an object first. results &lt;- data %&gt;% aov(data = ., swls2019_avg ~ marital_status) # you need to specify where the data is coming from. So, when you specify data = ., it&#39;s telling R that the data for analysis comes from the object data which is before the pipe operator, %&gt;%. # Look at the summary results summary(results) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## marital_status 2 10.78 5.392 9.624 8.6e-05 *** ## Residuals 340 190.47 0.560 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There are two other ways to run the ANOVA. The results will be exactly the same. # Alternative 1 results &lt;- aov(data$swls2019_avg ~ data$marital_status) #Alternative 2 results &lt;- aov(data = data, swls2019_avg ~ marital_status) Because the ANOVA results are statistically significant, we might want to follow up with pairwise comparisons (Tukey’s HSD). To run this, we need the DescTools package. (Remember to install the package Before continuing.) # Load package library(DescTools) # Perform a posthoc test on the results using Tukey&#39;s HSD. Oh, and give me the 95% CI. PostHocTest(results, method = &quot;hsd&quot;, conf.level = .95) ## ## Posthoc multiple comparisons of means : Tukey HSD ## 95% family-wise confidence level ## ## $marital_status ## diff lwr.ci upr.ci pval ## divorced-married -0.4233423 -0.65472757 -0.1919569 6.4e-05 *** ## widowed-married -0.2802413 -0.51321151 -0.0472710 0.0136 * ## widowed-divorced 0.1431010 -0.09184204 0.3780440 0.3247 ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If you want other posthoc tests, you can change the method portion. Type ?PostHocTest into the console to find out more about the different post hoc tests available to you. 8.7 Two-Way Between-Subjects ANOVA Suppose you want to know whether the difference in satisfaction with life scores between male and female depends on whether they have children. To find out, you will need to conduct a two-way between-subjects ANOVA. You’ll need the car package to run the ANOVA. # Load package library(car) # Change have_children into a factor data$have_children &lt;- factor(data$have_children, levels = c(0, 1), labels = c(&quot;no children&quot;, &quot;have children&quot;)) # Change contrasts settings when estimating Type-3 sum of squares options(contrasts = c(&#39;contr.sum&#39;, &#39;contr.poly&#39;)) # Set up the model being tested and store the model to [mod1] # The model has the following format: DV ~ IV1 + IV2 + IV1*IV2 mod1 &lt;- lm(swls2019_avg ~ gender + have_children + gender*have_children, data = data) # Conduct Two-Way ANOVA and get the ANOVA summary table # type = &quot;3&quot; refers to Type 3 sum of squares. For this class, we will use Type 3 sum of squares. Anova(mod1, type = &quot;3&quot;) ## Anova Table (Type III tests) ## ## Response: swls2019_avg ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 6366.9 1 13955.5140 &lt; 2.2e-16 *** ## gender 19.8 1 43.4025 1.703e-10 *** ## have_children 18.6 1 40.7208 5.767e-10 *** ## gender:have_children 3.1 1 6.7787 0.009631 ** ## Residuals 154.7 339 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Because the interaction is statistically significant, we will conduct the simple effects analysis. This is essentially like conducting independent groups t tests (with some corrections). Since we want to find out whether the difference between male and female’s satisfaction with life scores depends on having children, the two tests we will conduct would be: 1) difference in the satisfaction with life scores of males and females who have children; 2) difference in the satisfaction with life scores of males and females who do not have children. To do the simple effects analyses, we need the emmeans package. # Load package library(emmeans) # first, let&#39;s look at a plot # Separate Lines: have_children, X axis: gender emmip(mod1, have_children ~ gender) # Next, we conduct the simple effects investigating 1) the effect of gender (difference between males and females) for those who have children, and 2) the effect of gender for those who do not have children. emmeans(mod1, pairwise ~ gender | have_children) # comparing the pair of conditions in gender for each level of have_children ## $emmeans ## have_children = no children: ## gender emmean SE df lower.CL upper.CL ## male 3.95 0.0704 339 3.82 4.09 ## female 4.25 0.0770 339 4.10 4.40 ## ## have_children = have children: ## gender emmean SE df lower.CL upper.CL ## male 4.23 0.0775 339 4.08 4.38 ## female 4.91 0.0682 339 4.77 5.04 ## ## Confidence level used: 0.95 ## ## $contrasts ## have_children = no children: ## contrast estimate SE df t.ratio p.value ## male - female -0.292 0.104 339 -2.803 0.0054 ## ## have_children = have children: ## contrast estimate SE df t.ratio p.value ## male - female -0.675 0.103 339 -6.534 &lt;.0001 Suppose, though, you were interested in looking at the effect of having children by gender. In other words, you’re interested in looking at the difference 1) between men with and men without children, and 2) between women with and women without children. In this case, you will swap the variables around. # First, let&#39;s look at a plot # Separate Lines: gender, X axis: have_children emmip(mod1, gender ~ have_children) # Next, we conduct the simple effects investigating 1) the effect of having children (difference between having children and not having children) for males, and 2) of having children (difference between having children and not having children) for females. emmeans(mod1, pairwise ~ have_children | gender) # comparing the pair of conditions in have_children for each level of gender ## $emmeans ## gender = male: ## have_children emmean SE df lower.CL upper.CL ## no children 3.95 0.0704 339 3.82 4.09 ## have children 4.23 0.0775 339 4.08 4.38 ## ## gender = female: ## have_children emmean SE df lower.CL upper.CL ## no children 4.25 0.0770 339 4.10 4.40 ## have children 4.91 0.0682 339 4.77 5.04 ## ## Confidence level used: 0.95 ## ## $contrasts ## gender = male: ## contrast estimate SE df t.ratio p.value ## no children - have children -0.277 0.105 339 -2.648 0.0085 ## ## gender = female: ## contrast estimate SE df t.ratio p.value ## no children - have children -0.659 0.103 339 -6.410 &lt;.0001 8.8 Correlation Suppose you want to find out whether people who have higher satisfaction with life scores in 2019 also have higher satisfaction with life scores in 2021. Here is where you will conduct a correlation. data &lt;- data %&gt;% mutate(swls2021_avg = rowMeans(across(c(swls2021_1:swls2021_5)))) cor.test(data$swls2019_avg, data$swls2021_avg) ## ## Pearson&#39;s product-moment correlation ## ## data: data$swls2019_avg and data$swls2021_avg ## t = 38.396, df = 341, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.8792002 0.9193515 ## sample estimates: ## cor ## 0.9011917 Alternatively, you can do the correlation test this way: # Alternative 1 cor.test(~ swls2019_avg + swls2021_avg, data = data) # Alternative 2 data %&gt;% cor.test(formula = ~ swls2019_avg + swls2021_avg, .) All ways would give you the same result. 8.9 Regression Suppose you want to predict swls2021_avg from swls2019_avg. Here, we might want to conduct linear regression. The simplest form of linear regression is called simple linear regression. It only has two variables: one predictor and one outcome. # Conduct regression # the model takes the form of Y ~ X (where Y = outcome, X = predictor) data %&gt;% lm(swls2021_avg ~ swls2019_avg, data = .) %&gt;% summary(.) ## ## Call: ## lm(formula = swls2021_avg ~ swls2019_avg, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.05665 -0.24833 -0.00666 0.19334 0.80165 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.40659 0.09114 4.461 1.11e-05 *** ## swls2019_avg 0.79168 0.02062 38.396 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2925 on 341 degrees of freedom ## Multiple R-squared: 0.8121, Adjusted R-squared: 0.8116 ## F-statistic: 1474 on 1 and 341 DF, p-value: &lt; 2.2e-16 Unfortunately, the summary doesn’t give us the confidence interval. To get that, we need to specify that we want the confidence interval with the confint() function. # To get the 95% confidence interval data %&gt;% lm(swls2021_avg ~ swls2019_avg, data = .) %&gt;% confint(.) ## 2.5 % 97.5 % ## (Intercept) 0.2273200 0.5858564 ## swls2019_avg 0.7511261 0.8322388 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
